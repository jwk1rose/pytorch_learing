# Loss

## 什么是交叉熵损失函数，他有什么特点

> 交叉熵损失函数是一种常用的损失函数，主要用于分类任务中。
> 在多分类任务中，它可以度量模型输出的概率分布与真实的概率分布之间的差异。
> 它的主要特点是，当模型对某个类别预测的概率越接近于1时，交叉熵损失函数的值就越接近于0，
> 也就是说模型对该类别的分类效果越好；而当模型对某个类别的预测概率越接近于0时，交叉熵损失函数的值就越接近于正无穷大，
> 也就是说模型对该类别的分类效果越差。因此，在训练模型时，我们的目标是尽量减小交叉熵损失函数的值，从而提高模型的分类准确性。
> 交叉熵损失函数在反向传播算法中的梯度计算也比较简单，因此在深度学习中被广泛应用。
![img](./cross_entropy.png)

# Backward

## 什么是反向传播，为什么要反向传播

> 反向传播（Backpropagation）是一种常用于训练神经网络的算法。
> 它的基本思想是，通过计算神经网络中每个神经元的梯度信息，按照梯度下降的方向调整网络中的参数，从而最小化网络对训练数据的误差。
> 具体来说，反向传播算法首先通过前向计算得到网络的输出结果，然后根据训练数据的真实结果计算网络的误差（常用的误差函数包括均方误差、交叉熵等）。
> 接下来，它从后往前依次计算每个神经元的梯度信息，利用链式法则将误差逐层反向传递，最终得到网络中每个参数的梯度信息，
> 然后利用梯度下降算法进行参数更新。这样循环多次，就可以逐步调整网络参数，最终得到较好的分类效果。
> 反向传播算法的主要优点是，它可以有效地处理多层网络的梯度计算问题，而且具有较好的可扩展性和通用性。因此，它在深度学习领域得到了广泛的应用。

# Optimism

## 深度学习中有哪些优化器，它们分别有什么特点

> 深度学习中常用的优化器有以下几种：
> - SGD（Stochastic Gradient Descent，随机梯度下降）：是一种基础的优化算法，每次更新时只考虑单个样本的梯度并进行参数更新，可以有效地避免陷入局部最小值，但收敛速度较慢。
> - Momentum：在SGD算法中加入一个动量因子，累积之前的梯度信息，并加上当前的梯度信息进行参数更新，可以加快收敛速度。
> - AdaGrad（Adaptive Gradient，自适应梯度）：在SGD算法中对每个参数使用不同的学习率，将学习率根据历史梯度进行缩放，梯度较大的参数学习率较小，梯度较小的参数学习率较大，可以有效地处理稀疏数据。
> - RMSProp（Root Mean Square Propagation，均方根传播）：是对AdaGrad算法的改进，在计算梯度平方的平均值时采用滑动平均法，避免了AdaGrad算法中学习率过早衰减的问题。
>- Adam（Adaptive Moment Estimation，自适应矩估计）：结合了Momentum和RMSProp两种算法的优点，使用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率，适用于大规模数据和高维度的参数空间。
   每种优化器都有各自的特点，选择哪种优化器需要根据具体问题的特点进行综合考虑。
